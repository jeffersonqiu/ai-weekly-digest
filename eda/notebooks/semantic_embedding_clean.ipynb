{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0000001",
   "metadata": {},
   "source": [
    "# Feature Engineering & Semantic Embedding Pipeline\n",
    "\n",
    "This notebook builds features for citation-impact prediction and saves two versions:\n",
    "1. **Without embeddings** — numerical + one-hot features only\n",
    "2. **With embeddings** — adds sentence-transformer embeddings of title + abstract\n",
    "\n",
    "All saved parquets use **meaningful column names** and a companion JSON mapping file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000002",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Prevent PyTorch/libomp CPU segfaults on macOS\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.set_num_threads(1)\n",
    "torch.set_num_interop_threads(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000005",
   "metadata": {},
   "source": [
    "## Load & Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0000006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 102215 papers.\n",
      "popularity_bucket\n",
      "low     93538\n",
      "mid      4438\n",
      "high     3579\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/eda_papers.csv\")\n",
    "print(f\"Loaded {len(df)} papers.\")\n",
    "\n",
    "# Basic features\n",
    "df[\"published_at\"] = pd.to_datetime(df[\"published_at\"])\n",
    "df[\"age_days\"] = (pd.Timestamp.now(tz=\"UTC\") - df[\"published_at\"]).dt.days\n",
    "df[\"title_len\"] = df[\"title\"].apply(len)\n",
    "df[\"abstract_len\"] = df[\"abstract\"].apply(len)\n",
    "df[\"num_authors\"] = df[\"authors\"].apply(lambda x: len(str(x).split(\"|\")) if pd.notna(x) else 0)\n",
    "\n",
    "# Filter very new papers\n",
    "df = df[df[\"age_days\"] > 30]\n",
    "\n",
    "# Age binning\n",
    "df[\"age_bin\"] = (df[\"age_days\"] // 7) * 7\n",
    "df[\"age_bin\"] = pd.to_numeric(df[\"age_bin\"], errors=\"coerce\")\n",
    "df[\"citation_count\"] = pd.to_numeric(df[\"citation_count\"], errors=\"coerce\").fillna(0)\n",
    "df = df.dropna(subset=[\"age_bin\"]).copy()\n",
    "df[\"age_bin\"] = df[\"age_bin\"].astype(int)\n",
    "\n",
    "# Popularity bucket (target)\n",
    "degree = 1.08\n",
    "c1 = 0.019  # mid threshold\n",
    "c2 = 0.035  # high threshold\n",
    "\n",
    "mid_threshold = c1 * (df[\"age_bin\"] ** degree)\n",
    "high_threshold = c2 * (df[\"age_bin\"] ** degree)\n",
    "\n",
    "conditions = [\n",
    "    df[\"citation_count\"] < mid_threshold,\n",
    "    (df[\"citation_count\"] >= mid_threshold) & (df[\"citation_count\"] < high_threshold),\n",
    "    df[\"citation_count\"] >= high_threshold,\n",
    "]\n",
    "df[\"popularity_bucket\"] = np.select(conditions, [\"low\", \"mid\", \"high\"], default=\"low\")\n",
    "\n",
    "print(df[\"popularity_bucket\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000007",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0000008",
   "metadata": {},
   "outputs": [],
   "source": [
    "_WORD_RE = re.compile(r\"[A-Za-z0-9]+(?:'[A-Za-z0-9]+)?\")\n",
    "_SENT_SPLIT_RE = re.compile(r\"[.!?]+\\s+\")\n",
    "_URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "_EMAIL_RE = re.compile(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\")\n",
    "\n",
    "\n",
    "def add_offline_paper_features(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    title_col: str = \"title\",\n",
    "    abstract_col: str = \"abstract\",\n",
    "    authors_col: str = \"authors\",\n",
    "    primary_cat_col: str = \"primary_category\",\n",
    "    all_cats_col: str = \"all_categories\",\n",
    "    published_at_col: str = \"published_at\",\n",
    "    author_sep: str = \"|\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Add offline-only features for impact prediction.\"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    def safe_str(s) -> str:\n",
    "        return \"\" if pd.isna(s) else str(s)\n",
    "\n",
    "    def words(s: str):\n",
    "        return _WORD_RE.findall(s)\n",
    "\n",
    "    def sentence_count(s: str) -> int:\n",
    "        s = s.strip()\n",
    "        return 0 if not s else max(1, len(_SENT_SPLIT_RE.split(s)))\n",
    "\n",
    "    def keyword_flags(text_lower: str, patterns: dict) -> dict:\n",
    "        return {name: int(bool(re.search(pat, text_lower))) for name, pat in patterns.items()}\n",
    "\n",
    "    # ── Datetime ──\n",
    "    if published_at_col in out.columns:\n",
    "        dt = pd.to_datetime(out[published_at_col], errors=\"coerce\", utc=True)\n",
    "        out[\"pub_hour_utc\"] = dt.dt.hour\n",
    "        out[\"pub_dow\"] = dt.dt.dayofweek\n",
    "        out[\"pub_month\"] = dt.dt.month\n",
    "        out[\"is_weekend\"] = dt.dt.dayofweek.isin([5, 6]).astype(\"Int64\")\n",
    "\n",
    "    # ── Category features ──\n",
    "    if all_cats_col in out.columns:\n",
    "        cats = out[all_cats_col].fillna(\"\").astype(str)\n",
    "        out[\"num_categories\"] = cats.apply(lambda x: 0 if x.strip() == \"\" else len([c for c in x.split(\"|\") if c.strip()]))\n",
    "        out[\"is_cross_listed\"] = (out[\"num_categories\"] > 1).astype(\"Int64\")\n",
    "\n",
    "        def starts_with(prefix: str):\n",
    "            return cats.apply(lambda x: int(any(c.strip().startswith(prefix) for c in x.split(\"|\") if c.strip())))\n",
    "\n",
    "        out[\"has_cs\"] = starts_with(\"cs.\")\n",
    "        out[\"has_stat\"] = starts_with(\"stat.\")\n",
    "        out[\"has_math\"] = starts_with(\"math.\")\n",
    "        out[\"has_eess\"] = starts_with(\"eess.\")\n",
    "        out[\"has_qbio\"] = starts_with(\"q-bio.\")\n",
    "\n",
    "    if primary_cat_col in out.columns:\n",
    "        pc = out[primary_cat_col].fillna(\"\").astype(str)\n",
    "        out[\"primary_is_cs\"] = pc.str.startswith(\"cs.\").astype(\"Int64\")\n",
    "        out[\"primary_is_stat\"] = pc.str.startswith(\"stat.\").astype(\"Int64\")\n",
    "\n",
    "    # ── Author features ──\n",
    "    if authors_col in out.columns:\n",
    "        auth = out[authors_col].fillna(\"\").astype(str)\n",
    "        author_lists = auth.apply(lambda x: [a.strip() for a in x.split(author_sep) if a.strip()])\n",
    "        out[\"num_authors_offline\"] = author_lists.apply(len)\n",
    "        out[\"first_author\"] = author_lists.apply(lambda xs: xs[0] if len(xs) else \"\")\n",
    "        out[\"last_author\"] = author_lists.apply(lambda xs: xs[-1] if len(xs) else \"\")\n",
    "\n",
    "        author_name_lens = author_lists.apply(lambda xs: [len(a) for a in xs] if xs else [])\n",
    "        out[\"author_name_len_mean\"] = author_name_lens.apply(lambda ls: float(np.mean(ls)) if ls else np.nan)\n",
    "        out[\"author_name_len_max\"] = author_name_lens.apply(lambda ls: float(np.max(ls)) if ls else np.nan)\n",
    "        out[\"has_many_authors_ge5\"] = (out[\"num_authors_offline\"] >= 5).astype(\"Int64\")\n",
    "        out[\"has_many_authors_ge10\"] = (out[\"num_authors_offline\"] >= 10).astype(\"Int64\")\n",
    "\n",
    "    # ── Text features ──\n",
    "    title = out.get(title_col, \"\").apply(safe_str)\n",
    "    abstract = out.get(abstract_col, \"\").apply(safe_str)\n",
    "    title_lower = title.str.lower()\n",
    "    abs_lower = abstract.str.lower()\n",
    "\n",
    "    out[\"title_char_len\"] = title.str.len()\n",
    "    out[\"abstract_char_len\"] = abstract.str.len()\n",
    "    out[\"title_word_count\"] = title.apply(lambda s: len(words(s)))\n",
    "    out[\"abstract_word_count\"] = abstract.apply(lambda s: len(words(s)))\n",
    "    out[\"title_avg_word_len\"] = title.apply(lambda s: np.mean([len(w) for w in words(s)]) if words(s) else np.nan)\n",
    "    out[\"abstract_avg_word_len\"] = abstract.apply(lambda s: np.mean([len(w) for w in words(s)]) if words(s) else np.nan)\n",
    "    out[\"abstract_sentence_count\"] = abstract.apply(sentence_count)\n",
    "    out[\"abstract_avg_words_per_sentence\"] = (\n",
    "        out[\"abstract_word_count\"] / out[\"abstract_sentence_count\"].replace(0, np.nan)\n",
    "    )\n",
    "\n",
    "    def ratio_of(pattern: str, s: str) -> float:\n",
    "        return len(re.findall(pattern, s)) / max(1, len(s)) if s else 0.0\n",
    "\n",
    "    out[\"title_digit_ratio\"] = title.apply(lambda s: ratio_of(r\"\\d\", s))\n",
    "    out[\"abstract_digit_ratio\"] = abstract.apply(lambda s: ratio_of(r\"\\d\", s))\n",
    "    out[\"title_punct_ratio\"] = title.apply(lambda s: ratio_of(r\"[^\\w\\s]\", s))\n",
    "    out[\"abstract_punct_ratio\"] = abstract.apply(lambda s: ratio_of(r\"[^\\w\\s]\", s))\n",
    "\n",
    "    out[\"abstract_has_url\"] = abstract.apply(lambda s: int(bool(_URL_RE.search(s)))).astype(\"Int64\")\n",
    "    out[\"abstract_has_email\"] = abstract.apply(lambda s: int(bool(_EMAIL_RE.search(s)))).astype(\"Int64\")\n",
    "    out[\"mentions_github\"] = abs_lower.str.contains(\"github.com\", regex=False).astype(\"Int64\")\n",
    "    out[\"mentions_code\"] = abs_lower.str.contains(\"code\", regex=False).astype(\"Int64\")\n",
    "    out[\"mentions_dataset\"] = abs_lower.str.contains(\"dataset\", regex=False).astype(\"Int64\")\n",
    "    out[\"mentions_benchmark\"] = abs_lower.str.contains(\"benchmark\", regex=False).astype(\"Int64\")\n",
    "    out[\"mentions_arxiv_id\"] = abs_lower.str.contains(\"arxiv\", regex=False).astype(\"Int64\")\n",
    "    out[\"mentions_doi\"] = abs_lower.str.contains(\"doi\", regex=False).astype(\"Int64\")\n",
    "\n",
    "    # Keyword patterns\n",
    "    kw_patterns = {\n",
    "        \"is_survey\": r\"\\bsurvey\\b|\\breview\\b\",\n",
    "        \"is_benchmark_paper\": r\"\\bbenchmark\\b|\\bleaderboard\\b\",\n",
    "        \"is_dataset_paper\": r\"\\bdataset\\b|\\bcorpus\\b\",\n",
    "        \"is_system_paper\": r\"\\bsystem\\b|\\bframework\\b|\\bplatform\\b\",\n",
    "        \"has_theory\": r\"\\btheorem\\b|\\bproof\\b|\\bconvergence\\b\",\n",
    "        \"mentions_llm\": r\"\\bllm\\b|large language model|language model\",\n",
    "        \"mentions_diffusion\": r\"\\bdiffusion\\b\",\n",
    "        \"mentions_transformer\": r\"\\btransformer\\b\",\n",
    "        \"mentions_agent\": r\"\\bagent\\b|\\btool\\b|\\bplanning\\b\",\n",
    "        \"mentions_rl\": r\"\\breinforcement learning\\b|\\brl\\b\",\n",
    "        \"mentions_multimodal\": r\"\\bmultimodal\\b|vision-language|vlm\",\n",
    "        \"claims_sota\": r\"\\bsota\\b|state[- ]of[- ]the[- ]art\",\n",
    "        \"claims_novel\": r\"\\bnovel\\b|\\bnew\\b|\\bfirst\\b|\\bintroduce\\b\",\n",
    "        \"mentions_open_source\": r\"open[- ]source|we release|code is available\",\n",
    "        \"mentions_experiments\": r\"\\bexperiments?\\b|\\bwe evaluate\\b|\\bresults?\\b\",\n",
    "    }\n",
    "    combined_lower = (title_lower + \" \" + abs_lower).fillna(\"\")\n",
    "    kw_df = pd.DataFrame(\n",
    "        combined_lower.apply(lambda s: keyword_flags(s, kw_patterns)).tolist(),\n",
    "        index=out.index,\n",
    "    )\n",
    "    out = pd.concat([out, kw_df], axis=1)\n",
    "\n",
    "    # Lexical diversity\n",
    "    def ttr(s: str) -> float:\n",
    "        ws = [w.lower() for w in words(s)]\n",
    "        return len(set(ws)) / len(ws) if ws else np.nan\n",
    "\n",
    "    out[\"abstract_ttr\"] = abstract.apply(ttr)\n",
    "    out[\"log_abstract_word_count\"] = np.log1p(out[\"abstract_word_count\"])\n",
    "    out[\"log_num_authors\"] = np.log1p(out.get(\"num_authors_offline\", 0))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000009",
   "metadata": {},
   "source": [
    "## Build Model-Ready Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0000010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_df shape: (101555, 72)\n",
      "model_df: (51455, 72)\n",
      "X shape: (51455, 67)\n"
     ]
    }
   ],
   "source": [
    "# Apply feature engineering\n",
    "feat_df = add_offline_paper_features(df)\n",
    "print(\"feat_df shape:\", feat_df.shape)\n",
    "\n",
    "# Filter to papers with enough age for reliable citation signal\n",
    "feat_df[\"age_days\"] = pd.to_numeric(feat_df.get(\"age_days\", np.nan), errors=\"coerce\")\n",
    "model_df = feat_df[feat_df[\"age_days\"] >= 200].copy()\n",
    "print(\"model_df:\", model_df.shape)\n",
    "\n",
    "# Encode target: low=0, mid=1, high=2\n",
    "ordered_classes = [\"low\", \"mid\", \"high\"]\n",
    "y = pd.Categorical(model_df[\"popularity_bucket\"].astype(str), categories=ordered_classes, ordered=True).codes\n",
    "if (y == -1).any():\n",
    "    raise ValueError(\"Found unexpected popularity_bucket values outside low/mid/high.\")\n",
    "\n",
    "# Drop leak / ID columns\n",
    "leak_cols = {\"citation_count\", \"age_days\", \"age_bin\", \"popularity_bucket\"}\n",
    "id_cols = {\"arxiv_id\"}\n",
    "drop_cols = leak_cols | id_cols\n",
    "X = model_df.drop(columns=[c for c in drop_cols if c in model_df.columns]).copy()\n",
    "print(\"X shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000011",
   "metadata": {},
   "source": [
    "## Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0000012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (41164, 67)  X_test: (10291, 67)\n"
     ]
    }
   ],
   "source": [
    "TITLE_COL = \"title\"\n",
    "ABSTRACT_COL = \"abstract\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
    ")\n",
    "print(\"X_train:\", X_train.shape, \" X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000013",
   "metadata": {},
   "source": [
    "## Numerical Imputation & One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0000014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_cols: 59, ohe_cols: 124, total: 183\n"
     ]
    }
   ],
   "source": [
    "# Identify column groups\n",
    "cat_cols = [\"primary_category\"] if \"primary_category\" in X_train.columns else []\n",
    "num_cols = X_train.select_dtypes(include=[\"int64\", \"int32\", \"float64\", \"float32\", \"bool\"]).columns.tolist()\n",
    "num_cols = [c for c in num_cols if c not in {TITLE_COL, ABSTRACT_COL}]\n",
    "\n",
    "# Numerical imputation\n",
    "num_imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train_num = num_imputer.fit_transform(X_train[num_cols]) if num_cols else np.zeros((len(X_train), 0), np.float32)\n",
    "X_test_num  = num_imputer.transform(X_test[num_cols])      if num_cols else np.zeros((len(X_test),  0), np.float32)\n",
    "\n",
    "# One-hot encoding\n",
    "if cat_cols:\n",
    "    cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "    X_train_cat_raw = cat_imputer.fit_transform(X_train[cat_cols])\n",
    "    X_test_cat_raw  = cat_imputer.transform(X_test[cat_cols])\n",
    "\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    X_train_cat = ohe.fit_transform(X_train_cat_raw).astype(np.float32)\n",
    "    X_test_cat  = ohe.transform(X_test_cat_raw).astype(np.float32)\n",
    "    ohe_col_names = list(ohe.get_feature_names_out(cat_cols))\n",
    "else:\n",
    "    X_train_cat = np.zeros((len(X_train), 0), np.float32)\n",
    "    X_test_cat  = np.zeros((len(X_test),  0), np.float32)\n",
    "    ohe_col_names = []\n",
    "\n",
    "# Build the authoritative column-name list\n",
    "wo_emb_col_names = num_cols + ohe_col_names\n",
    "print(f\"num_cols: {len(num_cols)}, ohe_cols: {len(ohe_col_names)}, total: {len(wo_emb_col_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000015",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1 — Save Without Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0000016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Section 1 saved — X_train: (41164, 183), X_test: (10291, 183)\n",
      "   Columns: ['title_len', 'abstract_len', 'num_authors', 'pub_hour_utc', 'pub_dow'] ... (183 total)\n"
     ]
    }
   ],
   "source": [
    "SPLITS_DIR = \"./splits\"\n",
    "os.makedirs(SPLITS_DIR, exist_ok=True)\n",
    "\n",
    "# Build DataFrames with real column names\n",
    "X_train_wo_emb_df = pd.DataFrame(\n",
    "    np.hstack([X_train_num.astype(np.float32), X_train_cat]),\n",
    "    columns=wo_emb_col_names,\n",
    ")\n",
    "X_test_wo_emb_df = pd.DataFrame(\n",
    "    np.hstack([X_test_num.astype(np.float32), X_test_cat]),\n",
    "    columns=wo_emb_col_names,\n",
    ")\n",
    "\n",
    "y_train_df = pd.DataFrame(y_train, columns=[\"label\"])\n",
    "y_test_df  = pd.DataFrame(y_test,  columns=[\"label\"])\n",
    "\n",
    "# Save parquets\n",
    "X_train_wo_emb_df.to_parquet(os.path.join(SPLITS_DIR, \"X_train_wo_emb.parquet\"), engine=\"fastparquet\", index=False)\n",
    "X_test_wo_emb_df.to_parquet(os.path.join(SPLITS_DIR, \"X_test_wo_emb.parquet\"),   engine=\"fastparquet\", index=False)\n",
    "y_train_df.to_parquet(os.path.join(SPLITS_DIR, \"y_train.parquet\"), engine=\"fastparquet\", index=False)\n",
    "y_test_df.to_parquet(os.path.join(SPLITS_DIR, \"y_test.parquet\"),   engine=\"fastparquet\", index=False)\n",
    "\n",
    "# Save column mapping\n",
    "with open(os.path.join(SPLITS_DIR, \"column_names_wo_emb.json\"), \"w\") as f:\n",
    "    json.dump(wo_emb_col_names, f)\n",
    "\n",
    "print(f\"✅ Section 1 saved — X_train: {X_train_wo_emb_df.shape}, X_test: {X_test_wo_emb_df.shape}\")\n",
    "print(f\"   Columns: {wo_emb_col_names[:5]} ... ({len(wo_emb_col_names)} total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0000017",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2 — Generate Embeddings & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0000018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995d597e1a1c41428240910d1745b61a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress file mismatch; restarting from scratch.\n",
      "Encoding 4:2004 / 41164 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91d03d29906435d93a056dfe8dd0c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 2004:4004 / 41164 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e1836fa5444738a3668cd6f1e8058c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 4004:6004 / 41164 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e386efccc1646fa91e05f328e212256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 6004:8004 / 41164 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd8d989b6a64c94b4965dfad90e18ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 8004:10004 / 41164 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43a1c8124da4c13b7101feedc8a9d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 10004:12004 / 41164 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf632427053438392b141122c0b09c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 12004:14004 / 41164 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3bb1a51a1d94d89b0511c565bd2e311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 14004:16004 / 41164 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1713c54df4354411b81373bd34c37674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 16004:18004 / 41164 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807f8adf5dbd49cb9aeb73e89651af1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 18004:20004 / 41164 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5964e885fd4a0295db54dd30c7ca0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 20004:22004 / 41164 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2352c2496649909d591ddd4ef90947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 22004:24004 / 41164 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc42d3a82a8d4b7aa94e8402dac0f70c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 24004:26004 / 41164 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9291bf310e84de08f45fca028b8be1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 26004:28004 / 41164 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f69069a21b43bf8b58efb550c61d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 28004:30004 / 41164 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7eac68df68d471bb26430c144502c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 30004:32004 / 41164 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9702030f5c4483a8cc5533b34b33d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 32004:34004 / 41164 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca9a905b57547b883e3baf9c33013c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 34004:36004 / 41164 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a90c8a1e2b45c18ff0553fb6ae4484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 36004:38004 / 41164 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261129466e6b4e74a072e3e5203a3d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 38004:40004 / 41164 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5066ba4b3a704558869d28d45cd40f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 40004:41164 / 41164 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efcaffd1a96047078b8f5bdcc7677516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ade8cbefb7e4a4f9be7deb1b060634e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress file mismatch; restarting from scratch.\n",
      "Encoding 4:2004 / 10291 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d40e9fdcc54465aba0cdb209c7adac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 2004:4004 / 10291 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a122b88158a44978a6af7539ffaba392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 4004:6004 / 10291 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d053faafdd84fbf8a1de116471b81fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 6004:8004 / 10291 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845d01251abb4200b65ef30803fd0e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 8004:10004 / 10291 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac262b682654509b6edf4c9e4ddd234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 10004:10291 / 10291 ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140e47f9b61c4a91887cb27c003da408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_emb: (41164, 384)\n",
      "X_test_emb : (10291, 384)\n"
     ]
    }
   ],
   "source": [
    "EMB_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "CACHE_DIR = \"./emb_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "MAX_CHARS = 6000\n",
    "CHUNK_SIZE = 2000\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "\n",
    "def build_text(df_):\n",
    "    \"\"\"Concatenate title + abstract for embedding.\"\"\"\n",
    "    t = df_.get(TITLE_COL, \"\").fillna(\"\").astype(str)\n",
    "    a = df_.get(ABSTRACT_COL, \"\").fillna(\"\").astype(str)\n",
    "    return (t + \" [SEP] \" + a).str.slice(0, MAX_CHARS)\n",
    "\n",
    "\n",
    "def encode_resumable(texts: pd.Series, out_prefix: str, model_name: str):\n",
    "    \"\"\"\n",
    "    Encode texts to embeddings with resume support via memmap.\n",
    "    Writes: {out_prefix}.mmap (float32) and {out_prefix}.json (progress)\n",
    "    \"\"\"\n",
    "    texts = texts.fillna(\"\").astype(str)\n",
    "    n = len(texts)\n",
    "\n",
    "    prog_path = out_prefix + \".json\"\n",
    "    mmap_path = out_prefix + \".mmap\"\n",
    "\n",
    "    st = SentenceTransformer(model_name, device=DEVICE)\n",
    "\n",
    "    # Determine embedding dim\n",
    "    probe = st.encode(texts.iloc[:4].tolist(), batch_size=4, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    dim = probe.shape[1]\n",
    "\n",
    "    # Load or init progress\n",
    "    if os.path.exists(prog_path) and os.path.exists(mmap_path):\n",
    "        with open(prog_path, \"r\") as f:\n",
    "            prog = json.load(f)\n",
    "        start_idx = int(prog.get(\"done_until\", 0))\n",
    "        if prog.get(\"n\") != n or prog.get(\"dim\") != dim:\n",
    "            print(\"Progress file mismatch; restarting from scratch.\")\n",
    "            start_idx = 0\n",
    "            os.remove(prog_path)\n",
    "            os.remove(mmap_path)\n",
    "    else:\n",
    "        start_idx = 0\n",
    "\n",
    "    emb_mm = np.memmap(mmap_path, dtype=\"float32\", mode=\"w+\" if start_idx == 0 else \"r+\", shape=(n, dim))\n",
    "\n",
    "    if start_idx == 0:\n",
    "        emb_mm[:probe.shape[0], :] = probe.astype(np.float32)\n",
    "        start_idx = probe.shape[0]\n",
    "        emb_mm.flush()\n",
    "        with open(prog_path, \"w\") as f:\n",
    "            json.dump({\"n\": n, \"dim\": dim, \"done_until\": start_idx}, f)\n",
    "\n",
    "    for s in range(start_idx, n, CHUNK_SIZE):\n",
    "        e = min(s + CHUNK_SIZE, n)\n",
    "        print(f\"Encoding {s}:{e} / {n} ...\")\n",
    "        chunk = texts.iloc[s:e].tolist()\n",
    "        emb = st.encode(\n",
    "            chunk, batch_size=BATCH_SIZE, show_progress_bar=True,\n",
    "            convert_to_numpy=True, normalize_embeddings=True,\n",
    "        ).astype(np.float32)\n",
    "        emb_mm[s:e, :] = emb\n",
    "        emb_mm.flush()\n",
    "        with open(prog_path, \"w\") as f:\n",
    "            json.dump({\"n\": n, \"dim\": dim, \"done_until\": e}, f)\n",
    "\n",
    "    return np.array(emb_mm)\n",
    "\n",
    "\n",
    "# Build text & encode\n",
    "train_texts = build_text(X_train)\n",
    "test_texts  = build_text(X_test)\n",
    "\n",
    "X_train_emb = encode_resumable(train_texts, os.path.join(CACHE_DIR, \"train_emb\"), EMB_MODEL)\n",
    "X_test_emb  = encode_resumable(test_texts,  os.path.join(CACHE_DIR, \"test_emb\"),  EMB_MODEL)\n",
    "\n",
    "print(f\"X_train_emb: {X_train_emb.shape}\")\n",
    "print(f\"X_test_emb : {X_test_emb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0000019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Section 2 saved — X_train_final: (41164, 567), X_test_final: (10291, 567)\n",
      "   Columns: ['emb_0', 'emb_1', 'emb_2'] ... ['primary_category_stat.ME', 'primary_category_stat.ML', 'primary_category_stat.OT'] (567 total)\n"
     ]
    }
   ],
   "source": [
    "# Build column names for the final (with-embedding) version\n",
    "emb_dim = X_train_emb.shape[1]\n",
    "emb_col_names = [f\"emb_{i}\" for i in range(emb_dim)]\n",
    "final_col_names = emb_col_names + num_cols + ohe_col_names\n",
    "\n",
    "# Build DataFrames\n",
    "X_train_final_df = pd.DataFrame(\n",
    "    np.hstack([X_train_emb, X_train_num.astype(np.float32), X_train_cat]),\n",
    "    columns=final_col_names,\n",
    ")\n",
    "X_test_final_df = pd.DataFrame(\n",
    "    np.hstack([X_test_emb, X_test_num.astype(np.float32), X_test_cat]),\n",
    "    columns=final_col_names,\n",
    ")\n",
    "\n",
    "train_texts_df = pd.DataFrame(train_texts, columns=[\"text\"])\n",
    "test_texts_df  = pd.DataFrame(test_texts,  columns=[\"text\"])\n",
    "\n",
    "# Save parquets\n",
    "X_train_final_df.to_parquet(os.path.join(SPLITS_DIR, \"X_train_final.parquet\"), engine=\"fastparquet\", index=False)\n",
    "X_test_final_df.to_parquet(os.path.join(SPLITS_DIR, \"X_test_final.parquet\"),   engine=\"fastparquet\", index=False)\n",
    "train_texts_df.to_parquet(os.path.join(SPLITS_DIR, \"train_texts.parquet\"), engine=\"pyarrow\", index=False)\n",
    "test_texts_df.to_parquet(os.path.join(SPLITS_DIR, \"test_texts.parquet\"),   engine=\"pyarrow\", index=False)\n",
    "\n",
    "# Save column mapping\n",
    "with open(os.path.join(SPLITS_DIR, \"column_names_final.json\"), \"w\") as f:\n",
    "    json.dump(final_col_names, f)\n",
    "\n",
    "print(f\"✅ Section 2 saved — X_train_final: {X_train_final_df.shape}, X_test_final: {X_test_final_df.shape}\")\n",
    "print(f\"   Columns: {final_col_names[:3]} ... {final_col_names[-3:]} ({len(final_col_names)} total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f7bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
